{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook che definisce un autoencoder standard per ridurre le feature di MNIST. Per semplicità, ho caricato sulla repository l'encoder già addestrato per ridurre la dimensionalità a 64. È quindi sufficiente eseguire le celle che caricano \"autoencoder64ReLU.pth\" e che salvano i nuovi dataset, così da poterli utilizzare sull'altro notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni: converto in tensori e normalizzo\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.reshape(-1))])\n",
    "\n",
    "# Scarico i dataset\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "encoding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encod_dim):\n",
    "        super().__init__()\n",
    "        # Encoder: da 784 → encoding_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encod_dim),  # Dimensione compressa\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder: da encod_dim → 784\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encod_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Sigmoid()  # Output tra 0 e 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={784}, out_features={encoding_dim}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(encoding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    size = len(train_dataloader.dataset)\n",
    "    autoencoder.train()\n",
    "    for batch, (images, _) in enumerate(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = autoencoder(images)\n",
    "        loss = criterion(outputs, images) \n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(images)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(2, 6, figsize = (16,6))\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (img, _)  in enumerate(train_dataloader):\n",
    "        x = img[0].to(device)\n",
    "        pred = autoencoder(x)\n",
    "        ax[0,i].imshow(x.cpu().reshape(28,28)) \n",
    "        ax[1,i].imshow(pred.cpu().reshape(28,28))\n",
    "        if i==5:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), \"autoencoder64ReLU.pth\")\n",
    "print(\"Saved PyTorch Model State to autoencoder64ReLU.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(encoding_dim).to(device)\n",
    "autoencoder.load_state_dict(torch.load(\"autoencoder64ReLU.pth\", weights_only=True))\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(data_loader, model, padding = 0, dtype = torch.float64, device = 'cpu'):\n",
    "    model.eval()\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, y in data_loader:\n",
    "            images = images.to(device)\n",
    "            model = model.to(device)\n",
    "            compressed = model.encoder(images)\n",
    "            if padding:\n",
    "                compressed = torch.cat(\n",
    "                    [compressed.type(dtype),\n",
    "                    torch.zeros(compressed.shape[0], padding, device = device, dtype = dtype)\n",
    "                    ],\n",
    "                    dim = 1)   \n",
    "            labels.append(y.to(device))        \n",
    "            compressed_data.append(compressed)\n",
    "\n",
    "    return torch.cat(compressed_data, dim=0), torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encode_images(train_dataloader, autoencoder, padding = 10, dtype = torch.complex64), \"qsw_train_data_encoded64.pt\")\n",
    "torch.save(encode_images(test_dataloader, autoencoder, padding = 10, dtype = torch.complex64), \"qsw_test_data_encoded64.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
